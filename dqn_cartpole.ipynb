{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Dependencies\n",
    "import gym\n",
    "import random \n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from mish import Mish\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the cartpole environment \n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We extract the action space and state space from the environment\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We set hyperparameters and bunch of useful global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to tweak these parameters as we don't really know how they affect the performance of the dqn network\n",
    "gamma = 0.99\n",
    "epsilon_decay = 0.995\n",
    "epsilon_min = 0.001\n",
    "batch_size = 64\n",
    "train_start = 1000\n",
    "memory_size = 10000\n",
    "n_episodes = 400\n",
    "n_win_ticks = 195 # This value determine if the game is solved or not (score > n_win_ticks over 100 episodes)\n",
    "n_avg_scores = 100 # This value determine when the average score should be calculated (here every 100 episodes)\n",
    "render = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_29 (Dense)             (None, 128)               640       \n",
      "_________________________________________________________________\n",
      "mish_23 (Mish)               (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "mish_24 (Mish)               (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "mish_25 (Mish)               (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "mish_26 (Mish)               (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 11,538\n",
      "Trainable params: 11,538\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim = state_size))\n",
    "    model.add(Mish())\n",
    "    model.add(Dense(64))\n",
    "    model.add(Mish())\n",
    "    model.add(Dense(32))\n",
    "    model.add(Mish())\n",
    "    model.add(Dense(16))\n",
    "    model.add(Mish())\n",
    "    model.add(Dense(action_size))\n",
    "    \n",
    "    model.compile(Adam(lr = 0.001), loss = 'mse')\n",
    "    return model\n",
    "\n",
    "# Model summmary gives us a linear representation of our network\n",
    "model = build_model()\n",
    "target_model = build_model()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training helpers\n",
    "# Source: https://github.com/yanpanlau/CartPole/blob/master/DQN/CartPole_DQN.py\n",
    "def update_target_model():\n",
    "    target_model.set_weights(model.get_weights())\n",
    "\n",
    "def get_action(state, epsilon):\n",
    "    return np.random.randint(action_size) if np.random.rand() <= epsilon else np.argmax(model.predict(state)[0])\n",
    "\n",
    "def train_replay():\n",
    "    if len(memory) < train_start:\n",
    "        return\n",
    "    minibatch = random.sample(memory,  min(batch_size, len(memory)))\n",
    "    state_t, action_t, reward_t, state_t1, terminal = zip(*minibatch)\n",
    "    state_t = np.concatenate(state_t)\n",
    "    state_t1 = np.concatenate(state_t1)\n",
    "    targets = model.predict(state_t)\n",
    "    Q_sa = target_model.predict(state_t1)\n",
    "    targets[range(batch_size), action_t] = reward_t + gamma * np.max(Q_sa, axis=1) * np.invert(terminal)\n",
    "    model.train_on_batch(state_t, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode 0] Average Score: 29.0\n",
      "[Episode 100] Average Score: 126.35\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "avg_scores = []\n",
    "all_scores = []\n",
    "scores = deque(maxlen=n_avg_scores)\n",
    "memory = deque(maxlen=memory_size)\n",
    "\n",
    "def learn():\n",
    "    epsilon = 1.0 # Start with randomness\n",
    "    has_won = False\n",
    "\n",
    "    for e in range(n_episodes):\n",
    "        # reset environmenent after each episode\n",
    "        done = False\n",
    "        score = 0\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "\n",
    "        while not done:\n",
    "            action = get_action(state, epsilon)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "\n",
    "            memory.append((state, action, reward if not done else -100, next_state, done)) # give a penalty of 100 if the action taken makes the episode end\n",
    "            if epsilon > epsilon_min:\n",
    "                epsilon *= epsilon_decay # Decrease randomness\n",
    "            train_replay()\n",
    "            score += reward\n",
    "            state = next_state\n",
    "            \n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            if done:\n",
    "                env.reset()\n",
    "                update_target_model()\n",
    "                scores.append(score)\n",
    "                all_scores.append(score)\n",
    "                avg_score = np.mean(scores)\n",
    "                avg_scores.append(avg_score)\n",
    "\n",
    "                if not has_won and e >= n_avg_scores and avg_score >= n_win_ticks:\n",
    "                    # Find first score greater than 195 where the average is >= 195 over the following 100 episodes.\n",
    "                    solution_episode_idx = max(next(x[0] for x in enumerate(all_scores) if x[1] >= n_win_ticks), e - n_win_ticks) \n",
    "                    print('Solved after {} tries! \\o/'.format(solution_episode_idx))\n",
    "                    has_won = True\n",
    "\n",
    "                if e % n_avg_scores == 0:\n",
    "                    print('[Episode {}] Average Score: {}'.format(e, avg_score))\n",
    "\n",
    "learn()\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(all_scores, color='grey')\n",
    "plt.plot(avg_scores, color='red')\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
